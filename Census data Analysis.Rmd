---
title: "Tidy trial"
author: "Chrisphine"
date: "2024-07-04"
output: html_document
---

```{r}
library(mdsr)
library(nycflights13)
library(tidyverse)
library(ggplot2)
CH<-flights %>%
  filter(dest== "SFO",!is.na(arr_delay))
glimpse(flights)
glimpse(CH)
set.seed(101)
samplek<-CH%>%
  sample_n(size = 3000)
library(mosaic)
favstats(~arr_delay,data = samplek)
favstats(~dep_delay,data = samplek)
favstats(~arr_delay,data = CH)
#the 98th percentile of the arrival delays in our data sample
qdata(~arr_delay,p=0.98,data = samplek)
tally(~arr_delay<90,data=CH,format='proportion')
#With the population, itâ€™s easy to calculate the 98th percentile of the arrival delays
qdata(~arr_delay,p=0.98,data = CH)
Small<-sample_n(CH,size = 3,replace = FALSE)
#resampling the above sample
Small%>%sample_n(size = 3,replace = TRUE)
#bootstrapping to find the reliability of the mean arrival time calculated on asample of size 400.
n<-400
Orig_sample<-CH%>%sample_n(size = n,replace = FALSE)
#with the original sample in hand, we can draw a resample and calculate the mean arrival delay
mean(~arr_delay,data=sample_n(Orig_sample,size = n,replace = TRUE))
#Choosing an arbitrary value of 42minutes as a valuable marker for seriously delayed flights
(CH%>%
  filter(arr_delay>=420)%>%
  select(month,day,dep_delay,arr_delay,carrier))
CH%>%
  mutate(long_delay=arr_delay>60)%>%
  tally(~long_delay|month,data=.)
CH%>%filter(arr_delay<420)%>%
  ggplot(aes(arr_delay))+geom_histogram(binwidth = 20)
# the flights data frame has a variable (hour) that specifies the scheduled hour of departure
tally(~hour,data=CH)
CH%>%
  ggplot(aes(x=hour,y=arr_delay))+geom_boxplot(alpha=0.1,aes(group = hour))+geom_smooth(method = 'lm')+xlab('Scheduled hour of departure')+ylab('Arrival delay (minutes)')+coord_cartesian(ylim = c(-30,120))
#Statistical models
(mod1<-lm(arr_delay~hour,data = CH))
msummary(mod1)
library(lubridate)
CH<-CH%>%
  mutate(day=ymd(paste0(year,"-",month,"-",day)),
         dow=as.character(wday(day,label=TRUE)),
         season=ifelse(month%in% 6:7,"summer","other month"))
#build a model that includes variables we want to use to explain arrival delay
mod2<-lm(arr_delay~hour+origin+carrier+season+dow,data = CH)
msummary(mod2)
#Confounding and accounting for other factors
library(mdsr)
SAT_2010<-mutate(SAT_2010,salary=salary/1000)
(SAT_plot<-ggplot(data = SAT_2010,aes(x=salary,y=total))+geom_point()+geom_smooth(method = 'lm')+xlab('Average total score on SAT')+ylab('Average teacher salary(thousands of USD)'))
SAT_mod1<-lm(total~salary,data = SAT_2010)
msummary(SAT_mod1)
favstats(~sat_pct,data = SAT_2010)
SAT_2010<-SAT_2010%>%
  mutate(SAT_grp=ifelse(sat_pct<=27,'Low','High'))
tally(~SAT_grp,data=SAT_2010)
SAT_plot%+%SAT_2010+aes(colour = SAT_grp)
coef(lm(total~salary,data = filter(SAT_2010,SAT_grp=='Low')))
coef(lm(total~salary,data = filter(SAT_2010,SAT_grp=='High')))
SAT_mod2<-lm(total~salary+sat_pct,data = SAT_2010)
msummary(SAT_mod2)

## Classification
library(mdsr)
library(readr)
Census <- read_csv("C:/Users/Admin/Downloads/Census.csv")
View(Census)
any(is.na(Census))
names(Census)<-c("age","workclass","fnlwgt","education","education.num","marital.status","occupation","relationship","race","sex","capital.gain","capital.loss","hours.per.week","native.country","income")
#Having a glimpse of the dataset
library(tidyverse)
glimpse(Census)
#Splitting the dataset into training and test set
#Set seed for reproducibility
set.seed(1234)
N<-nrow(Census)
test_idx<-sample.int(N,size = round(0.2*N))
train<-Census[-test_idx,]
nrow(train)
test<-Census[test_idx,]
nrow(test)
#predicting income
library(mosaic)
library(ggplot2)
tally(~income,data=train,format='percent')
library(rpart)
rpart(income~capital.gain,data = train)
form<-as.formula("income ~ age + workclass + education + marital.status + occupation + relationship + race + sex + capital.gain + capital.loss + hours.per.week")
(mod_tree<-rpart(form,data = train))
plot(mod_tree)
text(mod_tree,use.n = TRUE,all = TRUE,cex=0.7)
library(partykit)
plot(as.party(mod_tree))
train<-train%>%
  mutate(husband_or_wife=relationship %in% c("Husband","Wife"),college_degree=husband_or_wife & education %in% c("Bachelors","Doctorate","Masters","Prof-scgool"),income_dtree=predict(mod_tree,type='class'))
cg_splits<-data.frame(husband_or_wife=c(TRUE,FALSE),vals=c(5095.5,7073.5))
ggplot(data = train,aes(x=capital.gain,y=income))+geom_count(aes(colour = income_dtree,shape = college_degree),position = position_jitter(width = 0,height = 0.3),alpha=0.5)+facet_wrap(~husband_or_wife)+geom_vline(data = cg_splits,aes(xintercept = vals),color='dodgerblue',lty=2)+scale_x_log10()
library(rpart.plot)
printcp(mod_tree)
train<-train%>%
  mutate(income_dtree=predict(mod_tree,type='class'))
(ConfusionMat<-tally(income_dtree~income,data=train,format='count'))
#Accuracy
sum(diag(ConfusionMat))/nrow(train)
#Parameter tuning
(mod_tree1<-rpart(form,data = train,control = rpart.control(cp=0.075)))
plot(mod_tree)
text(mod_tree1,use.n = TRUE,all = TRUE,cex=0.7)
plot(as.party(mod_tree1))
printcp(mod_tree1)
train<-train%>%
   mutate(income_dtree=predict(mod_tree1,type='class'))
(ConfusionMat1<-tally(income_dtree~income,data=train,format='count'))
#Accuracy
sum(diag(ConfusionMat1))/nrow(train)
#RandomForest
library(randomForest)
mod_forest<-randomForest(form,data = train,ntree=200,mtry=3)
mod_forest
sum(diag(mod_forest$confusion))/nrow(train)
library(tibble)
importance(mod_forest)%>%as.data.frame()%>%rownames_to_column()%>%arrange(desc(MeanDecreaseGini))
#Nearest neighbors
library(class)
#distance metric only works with quantitative variables
train_q<-train%>%
  select(age,education.num,capital.gain,capital.loss,hours.per.week)
Income_knn<-knn(train_q,test = train_q,cl=train$income,k=10)
(Confusion_knn<-tally(Income_knn~income,data=train,format='count'))
sum(diag(Confusion_knn))/nrow(train)
knn_error_rate<-function(x,y,numNeighbors,z=x){
  y_hat<-knn(train = x,test = z,cl=y,k=numNeighbors)
  return(sum(y_hat !=y)/nrow(x))
}
ks<-c(1:15,20,30,40,50)
train_rates<-sapply(ks,FUN = knn_error_rate,x=train_q,y=train$income)
knn_error_rates<-data.frame(k=ks,train_rate=train_rates)
ggplot(data = knn_error_rates,aes(x=k,y=train_rate))+geom_point()+geom_line()+ylab('Misclassification Rate')
#Naive Bayes
head(train,1)
library(e1071)
mod_nb<-naiveBayes(form,data = train)
income_nb<-predict(mod_nb,newdata = train)
(Confusion_nb<-tally(income_nb~income,data=train,format='count'))
sum(diag(Confusion_nb))/nrow(train)
library(nnet)
mod_nn<-nnet(form,data = train,size=5)
income_nn<-predict(mod_nn,newdata = train,type = 'class')
confusion_nn<-tally(income_nn~income,data=train,format='count')
confusion_nn
sum(diag(confusion_nn))/nrow(train)
##Ensemble methods
#using majority voting
income_ensemble<-ifelse((income_nb=='>50k')+
  (Income_knn=='>50k')>=2,">50k","<=50k")
Confusion_ensemble<-tally(income_ensemble~income,data=train,format='count')
Confusion_ensemble
sum(diag(Confusion_ensemble))/nrow(train)
##returning raw probabilities using the Naive Bayes
income_probs<-mod_nb%>%
  predict(newdata = train,type = 'raw')%>%
  as.data.frame()
head(income_probs,40)
names(income_probs)
tally(~'>50k' >0.5,data=income_probs,format='percent')
#use the overall observed percentage (i.e., 24%) as a threshold instead
tally(~'>50k'>0.24,data=income_probs,format='percent')
library(ROCR)
library(ggplot2)
pred<-ROCR::prediction(income_probs[,2],train$income)
perf<-ROCR::performance(pred,'tpr','fpr')
class(perf)
perf_df<-data.frame(perf@x.values,perf@y.values)
names(perf_df)<-c('fpr','tpr')
roc<-ggplot(data = perf_df,aes(x=fpr,y=tpr))+geom_line(color='dodgerblue')+geom_abline(intercept = 0,slope = 1,lty=3)+ylab(perf@y.name)+xlab(perf@x.name)
roc
(Confusion_nb<-tally(income_nb~income,data=train,format='count'))
sum(diag(Confusion_nb))/nrow(train)
tpr<- Confusion_nb[" >50K", " >50K"] / sum(Confusion_nb[, " >50K"])
fpr<- Confusion_nb[" >50K", " <=50K"] / sum(Confusion_nb[, " <=50K"])
roc + geom_point(x = fpr, y = tpr,size = 3)
#Bias-variance trade-off
test_q<-test%>%
  select(age,education.num,capital.gain,hours.per.week,capital.loss)
test_rates <- sapply(ks, FUN = knn_error_rate, x = train_q,
y = train$income, z = test_q)
knn_error_rates<-knn_error_rates%>%mutate(test_rate=test_rates)
library(tidyr)
knn_error_rates_tidy<-knn_error_rates%>%gather(key = 'type',value = 'error_rate',-k)
ggplot(data = knn_error_rates_tidy,aes(x=k,y=error_rate))+geom_point(aes(color=type))+geom_line(aes(color=type))+ylab('Misclassification Rate')
favstats(~capital.gain,data = train)
favstats(~capital.gain,data = test)
#Null model that simply predicts that everyone makes less than $50,000
train$income<-as.factor(train$income)
mod_null <- glm(income ~ 1, data = train, family = binomial)
mods<-list(mod_null,mod_tree,mod_forest,mod_nn,mod_nb)
lapply(mods,class)
predict_methods<-methods('predict')
predict_methods[grepl(pattern = '(glm|rpart|randomForest|nnet|naive)',predict_methods)]
predictions_train <- data.frame(
y = as.character(train$income),
type = "train",
mod_null = predict(mod_null, type = "response"),
mod_tree = predict(mod_tree, type = "class"),
mod_forest = predict(mod_forest, type = "class"),
mod_nn = predict(mod_nn, type = "class"),
mod_nb = predict(mod_nb, newdata = train, type = "class"))
predictions_test <- data.frame(
y = as.character(test$income),
type = "test",
mod_null = predict(mod_null, newdata = test, type = "response"),
mod_tree = predict(mod_tree, newdata = test, type = "class"),
mod_forest = predict(mod_forest, newdata = test, type = "class"),
mod_nn = predict(mod_nn, newdata = test, type = "class"),
mod_nb = predict(mod_nb, newdata = test, type = "class"))
predictions <- bind_rows(predictions_train, predictions_test)
glimpse(predictions)
predictions_tidy<-predictions%>%mutate(mod_null=ifelse(mod_null<0.5,'<=50K','>=50K'))%>%gather(key = 'model',value = 'y_hat',-type,-y)
glimpse(predictions_tidy)
#Now that we have the predictions for each model, we just need to compare them to the truth (y), and tally the results. We can do this using some dplyr machinations (note the use of the unite() function from the tidyr package)
predictions_summary <- predictions_tidy %>%
group_by(model, type) %>%
summarize(N = n(), correct = sum(y == y_hat, 0),
positives = sum(y == " >50K"),
true_pos = sum(y_hat == " >50K" & y == y_hat),
false_pos = sum(y_hat == " >50K" & y != y_hat)) %>%
mutate(accuracy = correct / N,
tpr = true_pos / positives,
fpr = false_pos / (N - positives)) %>%
ungroup() %>%
gather(val_type, val, -model, -type) %>%
unite(temp1, type, val_type, sep = "_") %>% # glue variables
spread(temp1, val) %>%
arrange(desc(test_accuracy)) %>%
select(model, train_accuracy, test_accuracy, test_tpr, test_fpr)
predictions_summary
#Comparing the roc-curves for all the five models
outputs<-c('response','prob','prob','raw','raw')
roc_tests<- mapply(predict, mods, type = outputs,
MoreArgs = list(newdata = test)) %>%
as.data.frame() %>%
select(1,3,5,6,8)
names(roc_tests) <-
c("mod_null", "mod_tree", "mod_forest", "mod_nn", "mod_nb")
glimpse(roc_tests)
get_roc <- function(x, y) {
pred <- ROCR::prediction(x$y_hat, y)
perf <- ROCR::performance(pred, 'tpr', 'fpr')
perf_df <- data.frame(perf@x.values, perf@y.values)
names(perf_df) <- c("fpr", "tpr")
return(perf_df)
}
roc_tidy<-roc_tests%>%gather(key = 'model',value = 'y_hat')%>%group_by(model)%>%dplyr::do(get_roc(., y=test$income))

ggplot(data = roc_tidy,aes(x=fpr,y=tpr))+geom_line(aes(color=model))+geom_abline(intercept = 0,slope = 1,lty=3)+ylab(perf@y.name)+xlab(perf@x.name)+geom_point(data = predictions_summary,size=3,aes(x=test_fpr,y=test_tpr,color=model))




```

